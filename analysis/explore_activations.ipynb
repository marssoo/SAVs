{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing \"training\" data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load desired benchmark activations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_paths = '../activations/'\n",
    "benchmark = 'MHalu'\n",
    "model = 'llava_ov_7b'\n",
    "\n",
    "path = os.path.join(activation_paths, benchmark + '+' + model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the train\n",
    "with open(path + '/train_activations.pkl', 'rb') as f:\n",
    "    activations = pickle.load(f)\n",
    "\n",
    "with open(path + '/train_classes.pkl', 'rb') as f:\n",
    "    labels_to_indices = pickle.load(f)\n",
    "\n",
    "#get reverse dictionnary (index to class)\n",
    "indices_to_labels = dict()\n",
    "for key, values in labels_to_indices.items():\n",
    "    for value in values:\n",
    "        indices_to_labels[value] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the centroids:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = dict()\n",
    "for label in labels_to_indices.keys():\n",
    "    centroids[label] = torch.zeros_like(activations[0]) #initialize null\n",
    "    \n",
    "    # go through associated indices (list of indices)\n",
    "    for i in labels_to_indices[label]:\n",
    "        centroids[label] += activations[i]\n",
    "    #average\n",
    "    centroids[label] /= len(labels_to_indices[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 784, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_str = dict()\n",
    "str_to_int = dict()\n",
    "# shape [num_classes, num_heads, dim_heads]\n",
    "class_activations = torch.zeros([len(centroids)] + list(activations[0].shape))\n",
    "\n",
    "for i, v in enumerate(centroids.keys()):\n",
    "    int_to_str[i] = v\n",
    "    str_to_int[v] = i\n",
    "    class_activations[i] = centroids[v]\n",
    "class_activations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting top heads\n",
    "Closely following the base implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_head_performance_base(class_activations, cur_activation, label, success_count):\n",
    "    \"\"\"\n",
    "    sample_activations: (num_sample, num_head, hidden_dim)\n",
    "    cur_activation: (num_head, hidden_dim)\n",
    "    success_count is dynamically updated\n",
    "    \"\"\"\n",
    "    #TODO change similarity here\n",
    "    all_sample = []\n",
    "\n",
    "    for i in range(class_activations.shape[1]):\n",
    "        scores = torch.nn.functional.cosine_similarity(class_activations[:, i, :], cur_activation[i, :], dim=-1)\n",
    "        all_sample.append(scores.argmax(dim=0).item())\n",
    "    for idx in range(len(all_sample)):\n",
    "        if all_sample[idx] == label:\n",
    "            success_count[idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 71.24it/s]\n"
     ]
    }
   ],
   "source": [
    "success_count = [0 for _ in range(class_activations.shape[1])]\n",
    "\n",
    "#go through training data\n",
    "for index, activation in tqdm(activations.items()):\n",
    "    int_label = str_to_int[indices_to_labels[index]]\n",
    "    record_head_performance_base(class_activations, activation, int_label, success_count)\n",
    "\n",
    "arr = np.array(success_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take $k$ best heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   4, 110, 147, 321, 322, 501, 525, 557, 563, 581, 584, 585,\n",
       "       756, 776])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#k = num_head\n",
    "k = 15 #How many top heads we want\n",
    "\n",
    "topk_indices = np.sort(np.argsort(arr)[-k:][::-1])\n",
    "topk_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate \"test\" set\n",
    "\n",
    "First, some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_heads(all_heads, topk_indices):\n",
    "    #assuming topk_indices sorted\n",
    "    k = len(topk_indices)\n",
    "    if len(all_heads.shape) == 3:\n",
    "        top_heads = torch.zeros((all_heads.shape[0], k, all_heads.shape[2]))\n",
    "        for i, k in enumerate(topk_indices):\n",
    "            top_heads[:, i, :] = all_heads[:, k, :]\n",
    "        return top_heads\n",
    "\n",
    "    elif len(all_heads.shape) != 2:\n",
    "        raise ValueError(\"Unrecognized shape for activations\")\n",
    "        \n",
    "    top_heads = torch.zeros((k, all_heads.shape[1]))\n",
    "    for i, k in enumerate(topk_indices):\n",
    "        top_heads[i, :] = all_heads[k, :]\n",
    "    return top_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_class_activations = get_top_heads(class_activations, topk_indices)\n",
    "top_class_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_examples(sample_activations, cur_activation):\n",
    "    \"\"\"sample_activations = class_activations limited to the top heads\"\"\"\n",
    "    all_sample = []\n",
    "    num_heads = cur_activation.shape[0]\n",
    "\n",
    "    for i in range(num_heads):\n",
    "        scores = torch.nn.functional.cosine_similarity(\n",
    "            sample_activations[:, i, :],  # (num_samples, hidden_dim)\n",
    "            cur_activation[i, :],         # (hidden_dim,)\n",
    "            dim=-1\n",
    "        )\n",
    "        all_sample.append(scores.argmax(dim=0).item())\n",
    "\n",
    "    counter = Counter(all_sample)\n",
    "    most_common = counter.most_common()\n",
    "\n",
    "    chosen_examples = [item[0] for item in most_common]\n",
    "    return chosen_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get number of test chunks\n",
    "number_of_chunks = 0\n",
    "for file in os.listdir(path):\n",
    "    if 'test_activations' in file:\n",
    "        number_of_chunks += 1\n",
    "\n",
    "#load ground_truth once and for all\n",
    "with open(path + '/test_classes.pkl', 'rb') as f:\n",
    "    test_labels_to_indices = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer chunk by chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through the test data by chunk :\n",
    "test = {}\n",
    "for i in range(number_of_chunks):\n",
    "    # load chunk\n",
    "    with open(path + f'/test_activations_{i}.pkl', 'rb') as f:\n",
    "        test_chunk = pickle.load(f)\n",
    "        test.update(test_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = len(test)\n",
    "results = np.zeros(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8123280132085856\n",
      "0.390450012376972\n"
     ]
    }
   ],
   "source": [
    "if benchmark == 'natural_ret':\n",
    "    # Compute binary predictions for each sample using the saved activations\n",
    "    results = np.zeros(test_size)  # 1D array to hold binary correctness per sample\n",
    "    for index, activations in test.items():\n",
    "        top_heads = get_top_heads(activations, topk_indices)  # only the heads we want\n",
    "        preds = retrieve_examples(top_class_activations, top_heads)\n",
    "        pred = preds[0]\n",
    "        str_class = int_to_str[pred]  # convert integer prediction to string label\n",
    "        # Set 1 if the current sample index is in the set of indices for the predicted label, else 0\n",
    "        results[index] = int(index in test_labels_to_indices[str_class])\n",
    "    \n",
    "    # Reshape results into groups of 4 for further aggregated metrics\n",
    "    group_results = results.reshape(-1, 4)\n",
    "    total_groups = group_results.shape[0]\n",
    "    \n",
    "    # Raw accuracy: average correctness across all samples\n",
    "    raw_acc = results.mean()\n",
    "    raw_std = results.std()\n",
    "    \n",
    "    # Question accuracy:\n",
    "    # For each group, the first question is correct if both sample 0 and 1 are correct,\n",
    "    # and the second question is correct if both sample 2 and 3 are correct.\n",
    "    q_first = group_results[:, 0] * group_results[:, 1]\n",
    "    q_second = group_results[:, 2] * group_results[:, 3]\n",
    "    q_correct = np.sum(q_first) + np.sum(q_second)\n",
    "    q_acc = q_correct / (total_groups * 2)  # Two questions per group\n",
    "    # Create a combined array of question outcomes per group (0 or 1 per question)\n",
    "    q_outcomes = np.concatenate((q_first, q_second))\n",
    "    q_std = q_outcomes.std()\n",
    "    \n",
    "    # Image accuracy:\n",
    "    # First image is correct if samples 0 and 2 are correct;\n",
    "    # Second image is correct if samples 1 and 3 are correct.\n",
    "    i_first = group_results[:, 0] * group_results[:, 2]\n",
    "    i_second = group_results[:, 1] * group_results[:, 3]\n",
    "    i_correct = np.sum(i_first) + np.sum(i_second)\n",
    "    i_acc = i_correct / (total_groups * 2)  # Two images per group\n",
    "    # Create a combined array of image outcomes per group (0 or 1 per image)\n",
    "    i_outcomes = np.concatenate((i_first, i_second))\n",
    "    i_std = i_outcomes.std()\n",
    "    \n",
    "    # Group accuracy: a group is correct if all four samples are correct.\n",
    "    g_outcomes = np.all(group_results == 1, axis=1).astype(float)\n",
    "    g_acc = g_outcomes.mean()\n",
    "    g_std = g_outcomes.std()\n",
    "    \n",
    "    # Print the metrics with standard deviations\n",
    "    print(f\"Raw Accuracy: {raw_acc:.4f}\") #± {raw_std:.4f}\")\n",
    "    print(f\"Question Accuracy: {q_acc:.4f}\") #± {q_std:.4f}\")\n",
    "    print(f\"Image Accuracy: {i_acc:.4f}\") #± {i_std:.4f}\")\n",
    "    print(f\"Group Accuracy: {g_acc:.4f}\") #± {g_std:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    results = np.zeros(test_size) # will hold results, we just need one dim\n",
    "    \n",
    "    for index, activations in test.items():\n",
    "        top_heads = get_top_heads(activations, topk_indices)        #only the heads we want\n",
    "        preds = retrieve_examples(top_class_activations, top_heads) \n",
    "        pred = preds[0]\n",
    "        str_class = int_to_str[pred]                              #string prediction\n",
    "        results[index] = int(index in test_labels_to_indices[str_class])  # 0 if the index isn't in the predicted class' indexes\n",
    "    \n",
    "    accuracy = results.mean()\n",
    "    std = results.std()\n",
    "    print(accuracy)\n",
    "    print(std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
