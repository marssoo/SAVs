{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from utils_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "validation = True\n",
    "validation_threshold = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/38 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../activations/pets+llava_ov_7b'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_paths = '../activations/'\n",
    "for folder in tqdm(os.listdir(activation_paths)):\n",
    "    benchmark, model = folder.split('+')\n",
    "    path = os.path.join(activation_paths, folder)\n",
    "    if benchmark == 'pets':\n",
    "        break\n",
    "\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, test_labels_to_indices, skipping = load_or_skip(path, validation, validation_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + '/train_activations.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open(path + '/train_classes.pkl', 'rb') as f:\n",
    "    train_labels_to_indices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices_to_labels(labels_to_indices):\n",
    "    indices_to_labels = dict()\n",
    "    for key, values in labels_to_indices.items():\n",
    "        for value in values:\n",
    "            indices_to_labels[value] = key\n",
    "    return indices_to_labels\n",
    "\n",
    "train_indices_to_labels = get_indices_to_labels(train_labels_to_indices)\n",
    "\n",
    "val_indices_to_labels = get_indices_to_labels(test_labels_to_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_dict_to_array(d):\n",
    "    \"\"\"\n",
    "    converts a dict of torch tensors to a numpy array of shape (num_samples_dim)\n",
    "    returns:\n",
    "         - X : the array (numpy)\n",
    "         - tensor_shapes : the shape information of the original tensors\n",
    "         - indices_correspondance : numpy array containing the indices of the \n",
    "                    corresponding dict keys (to not lose this information)\n",
    "    \"\"\"\n",
    "    tensor_shapes = d[list(d.keys())[0]].shape #shape of first tensor\n",
    "    n_samples = len(d)\n",
    "    total_dim = tensor_shapes[0] * tensor_shapes[1] # n_heads * dim\n",
    "    X = np.zeros((n_samples, total_dim))\n",
    "    indices_correspondence = np.zeros(n_samples) #array that will hold the true indices of tensors\n",
    "\n",
    "    for row_index, (tensor_index, tensor) in enumerate(d.items()):\n",
    "        indices_correspondence[row_index] = tensor_index\n",
    "        X[row_index] = d[tensor_index].float().flatten().numpy()\n",
    "\n",
    "    return X, tensor_shapes, indices_correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_L_last_layers(X, L=2):\n",
    "    \"\"\"for both 7B models, we have 28 heads of dim 128 per layer\"\"\"\n",
    "    if L is None: # condition to skip in order to facilitate gridsearch\n",
    "        return X\n",
    "    n_last_features = 128 * 28 * L\n",
    "    return X[:, - n_last_features:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 100352)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, tensor_shapes, train_ix = act_dict_to_array(train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, tensor_shapes, val_ix = act_dict_to_array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(indices_correspondence, indices_to_labels):\n",
    "    \"\"\"returns y associated to X, keeping the labels as strings (to use before a label encoder)\"\"\"\n",
    "    y = []\n",
    "    for i, tensor_ix in enumerate(indices_correspondence):\n",
    "        y.append(indices_to_labels[tensor_ix])\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['wheaten terrier', 'shiba inu', 'chihuahua', 'basset hound',\n",
       "       'Ragdoll'], dtype='<U26')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_val = get_y(train_ix, train_indices_to_labels), get_y(val_ix, val_indices_to_labels)\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_val_enc = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.0001, 0.001, 0.01, 0.1]}\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "grid_search = GridSearchCV(clf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter C: {'C': 0.001}\n",
      "Best cross-validation accuracy: 0.9\n",
      "Validation accuracy: 0.8303964757709251\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameter C:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# 6. Evaluate on the validation set\n",
    "y_val_pred = grid_search.predict(X_val)\n",
    "print(\"Validation accuracy:\", accuracy_score(y_val_enc, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49930087550524893"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = (y_val_enc == y_val_pred).std()\n",
    "std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupyr (block lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.473568281938326\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from groupyr import LogisticSGL  # pip install groupyr, needs to downgrade sklearn to 1.4 (--force-reinstall)\n",
    "\n",
    "# Assume X_train, X_val, y_train, and y_val are already defined.\n",
    "\n",
    "# Encode string labels to integers\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_val_enc = le.transform(y_val)\n",
    "\n",
    "# Define block structure:\n",
    "n_features = X_train.shape[1]\n",
    "block_size = 128  # based on heads\n",
    "n_groups = n_features // block_size\n",
    "\n",
    "# Create a 1D array with group assignments\n",
    "group_array = np.repeat(np.arange(n_groups), block_size)\n",
    "\n",
    "# If there are remaining features, assign them to an additional group.\n",
    "if n_features % block_size:\n",
    "    group_array = np.concatenate([group_array, \n",
    "                                  np.full(n_features % block_size, n_groups)])\n",
    "\n",
    "# Convert the 1D group_array into a list of arrays, each containing the indices for that group.\n",
    "groups = [np.where(group_array == g)[0] for g in np.unique(group_array)]\n",
    "\n",
    "# Instantiate a Logistic Regression estimator with Sparse Group Lasso penalty.\n",
    "# l1_ratio controls the mix between group lasso (l2 penalty on groups) and lasso (l1 penalty).\n",
    "clf = LogisticSGL(l1_ratio=.5, alpha=100, groups=groups, max_iter=1000)\n",
    "\n",
    "# Fit on the training data.\n",
    "clf.fit(X_train, y_train_enc)\n",
    "\n",
    "# Evaluate on the validation set.\n",
    "y_val_pred = clf.predict(X_val)\n",
    "print(\"Validation accuracy:\", accuracy_score(y_val_enc, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "savs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
