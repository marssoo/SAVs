Metadata-Version: 2.2
Name: SAVs
Version: 0.1.0
Summary: A Few-Shot Feature Extraction Method for LMMs to Perform Discriminative Vision-Language Tasks
Author-email: Chancharik Mitra <cmitra@cs.cmu.edu>, Brandon Huang <zhaobin@berkeley.edu>, "Tianning (Ray) Chai" <raychai@berkeley.edu>
Project-URL: homepage, https://github.com/yourusername/your-ml-package
Project-URL: repository, https://github.com/yourusername/your-ml-package
Project-URL: documentation, https://yourusername.github.io/your-ml-package
Keywords: deep learning,vision-language,feature extraction
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python :: 3.10
Classifier: License :: OSI Approved :: MIT License
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.0.1
Requires-Dist: torchvision>=0.15.2
Requires-Dist: numpy>=1.26.0
Requires-Dist: pandas>=2.2.0
Requires-Dist: transformers
Requires-Dist: scikit-learn>=1.5.0
Requires-Dist: matplotlib>=3.8.0
Requires-Dist: datasets>=2.19.0
Requires-Dist: timm>=0.9.0
Requires-Dist: aiohttp>=3.9.0
Requires-Dist: albumentations>=1.3.0
Requires-Dist: filelock>=3.13.0
Requires-Dist: huggingface-hub>=0.23.0
Requires-Dist: omegaconf>=2.3.0
Requires-Dist: diffusers>=0.20.0
Requires-Dist: deepspeed>=0.10.0
Requires-Dist: scipy>=1.11.0
Requires-Dist: pydantic>=1.10.0
Requires-Dist: xformers>=0.0.20
Requires-Dist: sentence-transformers>=3.0.0
Requires-Dist: open-clip-torch
Requires-Dist: flash-attn
Requires-Dist: baukit@ git+https://github.com/davidbau/baukit
Requires-Dist: llava@ git+https://github.com/LLaVA-VL/LLaVA-NeXT.git
Requires-Dist: qwen-vl-utils[decord]
Requires-Dist: accelerate==0.26.0

## # Running on SSP Cloud

1. Create a GPU instance such as `Vscode-pytorch-gpu` and launch with default parameters.

2. Open it.

3. Move `setup_env.sh` into the working directory.

4. Open a terminal.

5. Run the command:

   ```sh
   chmod u+x setup_env.sh && ./setup_env.sh
   ```

6. Wait until everything is done (~ 5 minutes). You can now run `run.sh` or create a notebook and start experimenting.

---

## Modifications to the main

- Added possibility to use `llava-onevision-qwen2-0.5b-ov` instead of `llava-onevision-qwen2-7b-ov`.
- Added `custom_builder.py` to allow it to run locally on 5.2 GB of vRAM.
- Added an executable `run.sh` allowing easy model swapping.

---

Running the llava models, see `run.sh` :

```sh
python3 -m src.run \
    --model_name llava_ov_0.5 \
    --data_name natural_ret \
    --train_path data/naturalbench_ret_train.jsonl \
    --val_path data/naturalbench_ret_test.jsonl
```

Switching `llava_ov_0.5` for `llava_ov_7b` will ignore all modifications and use the base code to run `llava-onevision-qwen2-7b-ov` as intended by the author.

---

Other Notes:
 - NaturalBench Images can be downloaded at the following [link](https://huggingface.co/datasets/BaiqiL/naturalbench_pictures/blob/main/raw_images.zip)

### üìù Citation
---
If you found our work useful, please consider starring and citing. Thank you!
```latex
@article{mitra2024sparse,
  title={Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers},
  author={Mitra, Chancharik and Huang, Brandon and Chai, Tianning and Lin, Zhiqiu and Arbelle, Assaf and Feris, Rogerio and Karlinsky, Leonid and Darrell, Trevor and Ramanan, Deva and Herzig, Roei},
  journal={arXiv preprint arXiv:2412.00142},
  year={2024}
}
```
